/*! For license information please see a263b7c0.17c338b2.js.LICENSE.txt */
"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[487],{3612:(e,n,t)=>{t.d(n,{Z:()=>u});var s=t(7294),i=t(6010),o=t(5281),a=t(5999);const r={admonition:"admonition_LlT9",admonitionHeading:"admonitionHeading_tbUL",admonitionIcon:"admonitionIcon_kALy",admonitionContent:"admonitionContent_S0QG"};var c=t(5108);const l={note:{infimaClassName:"secondary",iconComponent:function(){return s.createElement("svg",{viewBox:"0 0 14 16"},s.createElement("path",{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))},label:s.createElement(a.Z,{id:"theme.admonition.note",description:"The default label used for the Note admonition (:::note)"},"note")},tip:{infimaClassName:"success",iconComponent:function(){return s.createElement("svg",{viewBox:"0 0 12 16"},s.createElement("path",{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))},label:s.createElement(a.Z,{id:"theme.admonition.tip",description:"The default label used for the Tip admonition (:::tip)"},"tip")},danger:{infimaClassName:"danger",iconComponent:function(){return s.createElement("svg",{viewBox:"0 0 12 16"},s.createElement("path",{fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))},label:s.createElement(a.Z,{id:"theme.admonition.danger",description:"The default label used for the Danger admonition (:::danger)"},"danger")},info:{infimaClassName:"info",iconComponent:function(){return s.createElement("svg",{viewBox:"0 0 14 16"},s.createElement("path",{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))},label:s.createElement(a.Z,{id:"theme.admonition.info",description:"The default label used for the Info admonition (:::info)"},"info")},caution:{infimaClassName:"warning",iconComponent:function(){return s.createElement("svg",{viewBox:"0 0 16 16"},s.createElement("path",{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))},label:s.createElement(a.Z,{id:"theme.admonition.caution",description:"The default label used for the Caution admonition (:::caution)"},"caution")}},h={secondary:"note",important:"info",success:"tip",warning:"danger"};function d(e){const{mdxAdmonitionTitle:n,rest:t}=function(e){const n=s.Children.toArray(e),t=n.find((e=>s.isValidElement(e)&&"mdxAdmonitionTitle"===e.props?.mdxType)),i=s.createElement(s.Fragment,null,n.filter((e=>e!==t)));return{mdxAdmonitionTitle:t,rest:i}}(e.children);return{...e,title:e.title??n,children:t}}function u(e){const{children:n,type:t,title:a,icon:u}=d(e),m=function(e){const n=h[e]??e,t=l[n];return t||(c.warn(`No admonition config found for admonition type "${n}". Using Info as fallback.`),l.info)}(t),p=a??m.label,{iconComponent:f}=m,x=u??s.createElement(f,null);return s.createElement("div",{className:(0,i.Z)(o.k.common.admonition,o.k.common.admonitionType(e.type),"alert",`alert--${m.infimaClassName}`,r.admonition)},s.createElement("div",{className:r.admonitionHeading},s.createElement("span",{className:r.admonitionIcon},x),p),s.createElement("div",{className:r.admonitionContent},n))}},3414:(e,n,t)=>{t.d(n,{Z:()=>a});var s=t(7294),i=t(941),o=t(4996);const a=e=>{let{alt:n,sources:t,style:a}=e;const[r,c]=(0,s.useState)(!1),l=e=>{"Escape"===e.key&&c(!1)};(0,s.useEffect)((()=>(r?document.addEventListener("keydown",l):document.removeEventListener("keydown",l),()=>{document.removeEventListener("keydown",l)})),[r]);return s.createElement("div",{className:"zoomable-image "+(r?"fullscreen":""),onClick:()=>{c(!r)},style:{width:"50%",margin:"0 auto",display:"flex",justifyContent:"center",...a}},s.createElement(i.Z,{className:"zoomable-image-inner",alt:n,sources:{light:(0,o.Z)(t.light),dark:(0,o.Z)(t.dark)}}))}},4785:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>c,toc:()=>h});t(7294);var s=t(5893),i=t(1151),o=(t(941),t(4996),t(3414),t(3612));const a={},r="Chains",c={unversionedId:"components/chains",id:"components/chains",title:"Chains",description:"We appreciate your understanding as we polish our documentation \u2013 it may",source:"@site/docs/components/chains.mdx",sourceDirName:"components",slug:"/components/chains",permalink:"/components/chains",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Agents",permalink:"/components/agents"},next:{title:"Custom Components",permalink:"/components/custom"}},l={},h=[{value:"CombineDocsChain",id:"combinedocschain",level:3},{value:"ConversationChain",id:"conversationchain",level:3},{value:"ConversationalRetrievalChain",id:"conversationalretrievalchain",level:3},{value:"LLMChain",id:"llmchain",level:3},{value:"LLMMathChain",id:"llmmathchain",level:3},{value:"RetrievalQA",id:"retrievalqa",level:3},{value:"SQLDatabaseChain",id:"sqldatabasechain",level:3}];function d(e){const n=Object.assign({h1:"h1",p:"p",hr:"hr",h3:"h3",code:"code",a:"a",strong:"strong",ul:"ul",li:"li",em:"em"},(0,i.ah)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"chains",children:"Chains"}),"\n",(0,s.jsx)(o.Z,{type:"caution",icon:"\ud83d\udea7",title:"ZONE UNDER CONSTRUCTION",children:(0,s.jsx)("p",{children:(0,s.jsx)(n.p,{children:"We appreciate your understanding as we polish our documentation \u2013 it may\ncontain some rough edges. Share your feedback or report issues to help us\nimprove! \ud83d\udee0\ufe0f\ud83d\udcdd"})})}),"\n",(0,s.jsx)(n.p,{children:"Chains, in the context of language models, refer to a series of calls made to a language model. It allows for the output of one call to be used as the input for another call. Different types of chains allow for different levels of complexity. Chains are useful for creating pipelines and executing specific scenarios."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"combinedocschain",children:"CombineDocsChain"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"CombineDocsChain"})," incorporates methods to combine or aggregate loaded documents for question-answering functionality."]}),"\n",(0,s.jsx)(o.Z,{type:"info",children:(0,s.jsxs)(n.p,{children:["Works as a proxy of LangChain\u2019s ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/",children:"documents"})," chains generated by the ",(0,s.jsx)(n.code,{children:"load_qa_chain"})," function."]})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LLM:"})," Language Model to use in the chain."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"chain_type:"})," The chain type to be used. Each one of them applies a different \u201ccombination strategy\u201d."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"stuff"}),": The stuff ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/stuff",children:"documents"}),' chain (\u201cstuff" as in "to stuff" or "to fill") is the most straightforward of ',(0,s.jsx)(n.em,{children:"the"})," document chains. It takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM. This chain is well-suited for applications where documents are small and only a few are passed in for most calls."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"map_reduce"}),": The map-reduce ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/map_reduce",children:"documents"})," chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combined documents chain to get a single output (the Reduce step). It can optionally first compress or collapse the mapped documents to make sure that they fit in the combined documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"map_rerank"}),": The map re-rank ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/map_rerank",children:"documents"})," chain runs an initial prompt on each document that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest-scoring response is returned."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"refine"}),": The refine ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/refine",children:"documents"})," chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer."]}),"\n",(0,s.jsx)(n.p,{children:"Since the Refine chain only passes a single document to the LLM at a time, it is well-suited for tasks that require analyzing more documents than can fit in the model's context. The obvious tradeoff is that this chain will make far more LLM calls than, for example, the Stuff documents chain. There are also certain tasks that are difficult to accomplish iteratively. For example, the Refine chain can perform poorly when documents frequently cross-reference one another or when a task requires detailed information from many documents."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"conversationchain",children:"ConversationChain"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ConversationChain"})," is a straightforward chain for interactive conversations with a language model, making it ideal for chatbots or virtual assistants. It allows for dynamic conversations, question-answering, and complex dialogues."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM:"})," Language Model to use in the chain."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory:"})," Default memory store."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"input_key:"})," Used to specify the key under which the user input will be stored in the conversation memory. It allows you to provide the user's input to the chain for processing and generating a response."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"output_key:"})," Used to specify the key under which the generated response will be stored in the conversation memory. It allows you to retrieve the response using the specified key."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"verbose:"})," This parameter is used to control the level of detail in the output of the chain. When set to True, it will print out some internal states of the chain while it is being run, which can be helpful for debugging and understanding the chain's behavior. If set to False, it will suppress the verbose output \u2014 defaults to ",(0,s.jsx)(n.code,{children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"conversationalretrievalchain",children:"ConversationalRetrievalChain"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ConversationalRetrievalChain"})," extracts information and provides answers by combining document search and question-answering abilities."]}),"\n",(0,s.jsx)(o.Z,{type:"info",children:(0,s.jsx)(n.p,{children:"A retriever is a component that finds documents based on a query. It doesn't store the documents themselves, but it returns the ones that match the query."})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LLM:"})," Language Model to use in the chain."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Memory:"})," Default memory store."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Retriever:"})," The retriever used to fetch relevant documents."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"chain_type:"})," The chain type to be used. Each one of them applies a different \u201ccombination strategy\u201d."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"stuff"}),": The stuff ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/stuff",children:"documents"}),' chain (\u201cstuff" as in "to stuff" or "to fill") is the most straightforward of ',(0,s.jsx)(n.em,{children:"the"})," document chains. It takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM. This chain is well-suited for applications where documents are small and only a few are passed in for most calls."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"map_reduce"}),": The map-reduce ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/map_reduce",children:"documents"})," chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combined documents chain to get a single output (the Reduce step). It can optionally first compress or collapse the mapped documents to make sure that they fit in the combined documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"map_rerank"}),": The map re-rank ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/map_rerank",children:"documents"})," chain runs an initial prompt on each document that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest-scoring response is returned."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"refine"}),": The refine ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/chains/document/refine",children:"documents"})," chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer."]}),"\n",(0,s.jsx)(n.p,{children:"Since the Refine chain only passes a single document to the LLM at a time, it is well-suited for tasks that require analyzing more documents than can fit in the model's context. The obvious tradeoff is that this chain will make far more LLM calls than, for example, the Stuff documents chain. There are also certain tasks that are difficult to accomplish iteratively. For example, the Refine chain can perform poorly when documents frequently cross-reference one another or when a task requires detailed information from many documents."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"return_source_documents:"})," Used to specify whether or not to include the source documents that were used to answer the question in the output. When set to ",(0,s.jsx)(n.code,{children:"True"}),", source documents will be included in the output along with the generated answer. This can be useful for providing additional context or references to the user \u2014 defaults to ",(0,s.jsx)(n.code,{children:"True"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"verbose:"})," Whether or not to run in verbose mode. In verbose mode, intermediate logs will be printed to the console \u2014 defaults to ",(0,s.jsx)(n.code,{children:"False"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"llmchain",children:"LLMChain"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"LLMChain"})," is a straightforward chain that adds functionality around language models. It combines a prompt template with a language model. To use it, create input variables to format the prompt template. The formatted prompt is then sent to the language model, and the generated output is returned as the result of the ",(0,s.jsx)(n.code,{children:"LLMChain"}),"."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM:"})," Language Model to use in the chain."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory:"})," Default memory store."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt"}),": Prompt template object to use in the chain."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"output_key:"})," This parameter is used to specify which key in the LLM output dictionary should be returned as the final output. By default, the ",(0,s.jsx)(n.code,{children:"LLMChain"})," returns both the input and output key values \u2014 defaults to ",(0,s.jsx)(n.code,{children:"text"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"verbose:"})," Whether or not to run in verbose mode. In verbose mode, intermediate logs will be printed to the console \u2014 defaults to ",(0,s.jsx)(n.code,{children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"llmmathchain",children:"LLMMathChain"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"LLMMathChain"})," combines a language model (LLM) and a math calculation component. It allows the user to input math problems and get the corresponding solutions."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"LLMMathChain"})," works by using the language model with an ",(0,s.jsx)(n.code,{children:"LLMChain"})," to understand the input math problem and generate a math expression. It then passes this expression to the math component, which evaluates it and returns the result."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM:"})," Language Model to use in the chain."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLMChain:"})," LLM Chain to use in the chain."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory:"})," Default memory store."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"input_key:"})," Used to specify the input value for the mathematical calculation. It allows you to provide the specific values or variables that you want to use in the calculation \u2014 defaults to ",(0,s.jsx)(n.code,{children:"question"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"output_key:"})," Used to specify the key under which the output of the mathematical calculation will be stored. It allows you to retrieve the result of the calculation using the specified key \u2014 defaults to ",(0,s.jsx)(n.code,{children:"answer"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"verbose:"})," Whether or not to run in verbose mode. In verbose mode, intermediate logs will be printed to the console \u2014 defaults to ",(0,s.jsx)(n.code,{children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"retrievalqa",children:"RetrievalQA"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"RetrievalQA"})," is a chain used to find relevant documents or information to answer a given query. The retriever is responsible for returning the relevant documents based on the query, and the QA component then extracts the answer from those documents. The retrieval QA system combines the capabilities of both the retriever and the QA component to provide accurate and relevant answers to user queries."]}),"\n",(0,s.jsx)(o.Z,{type:"info",children:(0,s.jsx)(n.p,{children:"A retriever is a component that finds documents based on a query. It doesn't store the documents themselves, but it returns the ones that match the query."})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Combine Documents Chain:"})," Chain to use to combine the documents."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory:"})," Default memory store."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Retriever:"})," The retriever used to fetch relevant documents."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"input_key:"})," This parameter is used to specify the key in the input data that contains the question. It is used to retrieve the question from the input data and pass it to the question-answering model for generating the answer \u2014 defaults to ",(0,s.jsx)(n.code,{children:"query"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"output_key:"})," This parameter is used to specify the key in the output data where the generated answer will be stored. It is used to retrieve the answer from the output data after the question-answering model has generated it \u2014 defaults to ",(0,s.jsx)(n.code,{children:"result"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"return_source_documents:"})," Used to specify whether or not to include the source documents that were used to answer the question in the output. When set to ",(0,s.jsx)(n.code,{children:"True"}),", source documents will be included in the output along with the generated answer. This can be useful for providing additional context or references to the user \u2014 defaults to ",(0,s.jsx)(n.code,{children:"True"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"verbose:"})," Whether or not to run in verbose mode. In verbose mode, intermediate logs will be printed to the console \u2014 defaults to ",(0,s.jsx)(n.code,{children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"sqldatabasechain",children:"SQLDatabaseChain"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"SQLDatabaseChain"})," finds answers to questions using a SQL database. It works by using the language model to understand the SQL query and generate the corresponding SQL code. It then passes the SQL code to the SQL database component, which executes the query on the database and returns the result."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Db:"})," SQL Database to connect to."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM:"})," Language Model to use in the chain."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt:"})," Prompt template to translate natural language to SQL."]}),"\n"]})]})}const u=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,i.ah)(),e.components);return n?(0,s.jsx)(n,Object.assign({},e,{children:(0,s.jsx)(d,e)})):d(e)}},5251:(e,n,t)=>{t(7418);var s=t(7294),i=60103;if(n.Fragment=60107,"function"==typeof Symbol&&Symbol.for){var o=Symbol.for;i=o("react.element"),n.Fragment=o("react.fragment")}var a=s.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,r=Object.prototype.hasOwnProperty,c={key:!0,ref:!0,__self:!0,__source:!0};function l(e,n,t){var s,o={},l=null,h=null;for(s in void 0!==t&&(l=""+t),void 0!==n.key&&(l=""+n.key),void 0!==n.ref&&(h=n.ref),n)r.call(n,s)&&!c.hasOwnProperty(s)&&(o[s]=n[s]);if(e&&e.defaultProps)for(s in n=e.defaultProps)void 0===o[s]&&(o[s]=n[s]);return{$$typeof:i,type:e,key:l,ref:h,props:o,_owner:a.current}}n.jsx=l,n.jsxs=l},5893:(e,n,t)=>{e.exports=t(5251)},1151:(e,n,t)=>{t.d(n,{Zo:()=>r,ah:()=>o});var s=t(7294);const i=s.createContext({});function o(e){const n=s.useContext(i);return s.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const a={};function r({components:e,children:n,disableParentContext:t}){let r;return r=t?"function"==typeof e?e({}):e||a:o(e),s.createElement(i.Provider,{value:r},n)}}}]);